{"cells":[{"cell_type":"markdown","source":["### Przetwarzanie danych z sondaży aerologicznych\n\nŹródło danych: http://weather.uwyo.edu/upperair/sounding.html\n\nDokonano ekstrakcji danych pomiarowych dla zakresu dat: 2013.06.01 - 2017.05.31\n\nIlość rekordów wynosi: 273 497\n\nKażdy wiersz danych składa się z 6 pól:\n* data i godzina pomiaru\n* ciśnienie\n* wysokość balonu\n* temperatura powietrza\n* temperatura punktu rosy\n* wilgotność względna"],"metadata":{}},{"cell_type":"markdown","source":["##### Załadowanie zewnętrznych danych\n\nPierwszym krokiem jest załadowanie zewnętrznych danych, aby utworzyć RDD (Resilient Distributed Dataset). Jest to abstrakcyjny typ danych używany w Apache Spark jako podstawowy."],"metadata":{}},{"cell_type":"code","source":["meteo_rdd = sc.textFile(\"/FileStore/tables/el11t8q01497432769031/data_csv-1aa57.gz\")\nprint('Number of raw records: {}'.format(meteo_rdd.count()))\n\n# podziel kazda linie na poszczegolne pola \nsplitted_lines = meteo_rdd.map(lambda l: l.split(\";\"))\ndef group_height(h):\n  if h <= 250:\n    return(1)\n  elif h > 250 and h <= 2000:\n    return(2)\n  elif h > 2000:\n    return(int(h / 6000) + 3)\n  else:\n    return(0)\n\n# zrzutuj podzielone pola na odpowiednie typy i stworz krotke (tuple)\nmeasures = splitted_lines.map(lambda p: (int(\"20\" + p[0][:2]), int(p[0][2:4]), int(p[0][4:6]), float(p[1]), int(p[2]), float(p[3]), float(p[4]), int(p[5]), group_height(int(p[2])), str(p[0][:6])))\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["##### Wykonanie agregacji na RDD\n\nPoczątkowym etapem jest wyodrębnienie osobnych części RDD ze zbioru danych za pomocą operacji filter(). Do sprawdzenia warunku zadeklarowano prostą funkcję, która sprawdza czy wysokość mieści się w zadanym przedziale. Za pomocą operacji map() pobieramy pomiary ciśnienia atmosferycznego. Następnie za pomocą funkcji reduce() możemy policzyć całkowite ciśnienie na danej wysokości i podzielić tą sumę, aby otrzymać uśrednione wartości.\n\n###### 1) za pomocą reduce()"],"metadata":{}},{"cell_type":"code","source":["# funkcja do filtrowania wysokosci\ndef h_is_near(h, hp, r):\n  if h >= hp - r and h <= hp + r:\n    return(True)\n  else:\n    return(False)\n\n# wyodrebnienie danych RDD i pobranie cisnienia\npressure_near_ground = measures.filter(lambda x: h_is_near(int(x[4]), 100, 50)).map(lambda x: int(x[3]))\npressure_near_1km = measures.filter(lambda x: h_is_near(int(x[4]), 1000, 100)).map(lambda x: int(x[3]))\npressure_near_5km = measures.filter(lambda x: h_is_near(int(x[4]), 5000, 500)).map(lambda x: int(x[3]))\npressure_near_15km = measures.filter(lambda x: h_is_near(int(x[4]), 15000, 1500)).map(lambda x: int(x[3]))\npressure_near_30km = measures.filter(lambda x: h_is_near(int(x[4]), 30000, 3000)).map(lambda x: int(x[3]))\n\n# obliczenie calkowitej liczby probek\nnear_ground_count = pressure_near_ground.count()\nnear_1km_count = pressure_near_1km.count()\nnear_5km_count = pressure_near_5km.count()\nnear_15km_count = pressure_near_15km.count()\nnear_30km_count = pressure_near_30km.count()\n\n# agregacje\ntotal_pressure_near_ground = pressure_near_ground.reduce(lambda x, y: x+y)\ntotal_pressure_near_1km = pressure_near_1km.reduce(lambda x, y: x+y)\ntotal_pressure_near_5km = pressure_near_5km.reduce(lambda x, y: x+y)\ntotal_pressure_near_15km = pressure_near_15km.reduce(lambda x, y: x+y)\ntotal_pressure_near_30km = pressure_near_30km.reduce(lambda x, y: x+y)\n\n# wyniki\nprint(\"Average pressure near the ground:\\t{}\".format(total_pressure_near_ground/near_ground_count))\nprint(\"Average pressure at 1 km:\\t{}\".format(total_pressure_near_1km/near_1km_count))\nprint(\"Average pressure at 5 km:\\t{}\".format(total_pressure_near_5km/near_5km_count))\nprint(\"Average pressure at 15 km:\\t{}\".format(total_pressure_near_15km/near_15km_count))\nprint(\"Average pressure at 30 km:\\t{}\".format(total_pressure_near_30km/near_30km_count))\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["###### 2) za pomocą aggregate()\n\nKrótszy kod, wykonanie kilku operacji w każdej iteracji co jest około 2x szybsze."],"metadata":{}},{"cell_type":"code","source":["# funkcja do filtrowania wysokosci\ndef h_is_near(h, hp, r):\n  if h >= hp - r and h <= hp + r:\n    return(True)\n  else:\n    return(False)\n\n# funkcja dodajaca wartosc do akumulatora\ndef add2acc(acc, value):\n  return(acc[0] + value, acc[1] + 1)\n\n# funkcja dodajaca dwa akumulatory\ndef addaccs(acc1, acc2):\n  return(acc1[0] + acc2[0], acc1[1] + acc2[1])\n\n# wyodrebnienie danych RDD i pobranie cisnienia\npressure_near_ground = measures.filter(lambda x: h_is_near(int(x[4]), 100, 50)).map(lambda x: int(x[3]))\npressure_near_1km = measures.filter(lambda x: h_is_near(int(x[4]), 1000, 100)).map(lambda x: int(x[3]))\npressure_near_5km = measures.filter(lambda x: h_is_near(int(x[4]), 5000, 500)).map(lambda x: int(x[3]))\npressure_near_15km = measures.filter(lambda x: h_is_near(int(x[4]), 15000, 1500)).map(lambda x: int(x[3]))\npressure_near_30km = measures.filter(lambda x: h_is_near(int(x[4]), 30000, 3000)).map(lambda x: int(x[3]))\n\n# agregacje (calkowite cisnienie, calkowita liczba probek)\naggr_near_ground = pressure_near_ground.aggregate((0, 0), add2acc, addaccs)\naggr_near_1km = pressure_near_1km.aggregate((0, 0), add2acc, addaccs)\naggr_near_5km = pressure_near_5km.aggregate((0, 0), add2acc, addaccs)\naggr_near_15km = pressure_near_15km.aggregate((0, 0), add2acc, addaccs)\naggr_near_30km = pressure_near_30km.aggregate((0, 0), add2acc, addaccs)\n\n# wyniki\nprint(\"Average pressure near the ground:\\t{}\".format(aggr_near_ground[0]/aggr_near_ground[1]))\nprint(\"Average pressure at 1 km:\\t{}\".format(aggr_near_1km[0]/aggr_near_1km[1]))\nprint(\"Average pressure at 5 km:\\t{}\".format(aggr_near_5km[0]/aggr_near_5km[1]))\nprint(\"Average pressure at 15 km:\\t{}\".format(aggr_near_15km[0]/aggr_near_15km[1]))\nprint(\"Average pressure at 30 km:\\t{}\".format(aggr_near_30km[0]/aggr_near_30km[1]))\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["##### Przekształcenie RDD do DataFrame"],"metadata":{}},{"cell_type":"code","source":["meteo_df = spark.createDataFrame(measures, ['year', 'month', 'day', 'pressure', 'height', 'temp', 'dewpoint', 'humidity', 'height_group', 'datestamp'])\nmeteo_df.printSchema()\nmeteo_df.show()\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["##### Wykonanie agregacji na DF\n\nZ wykorzystaniem DSL i SQL."],"metadata":{}},{"cell_type":"code","source":["# ilosc pomiarow w poszczegolnych latach ponizej zadanej wysokosci\nmeteo_df.filter(meteo_df['height'] < 150).groupBy('year').count().orderBy('year').show() \n\nmeteo_df.createOrReplaceTempView(\"meteo\")\nspark.sql(\"SELECT year, count(*) FROM meteo WHERE height < 150 GROUP BY year ORDER BY year\").show()\n\naverages_in_months_and_heights = meteo_df.groupBy('month','height_group').avg('temp','pressure','humidity','dewpoint').orderBy('month','height_group', ascending=True)\nmaximums_in_months_and_heights = meteo_df.groupBy('month','height_group').max('temp','pressure','humidity','dewpoint').orderBy('month','height_group', ascending=True)\n\nfrom pyspark.sql.functions import mean, min, max, col\naggregate = ['temp','pressure','humidity','dewpoint'] \nfuns = [mean, min, max]\nexprs = [f(col(c)) for f in funs for c in aggregate]\n\nstats_in_months_and_heights = meteo_df.groupBy('month','height_group').agg(*exprs).orderBy('month','height_group', ascending=True)\n\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["##### Wizualizacja danych"],"metadata":{}},{"cell_type":"code","source":["display(stats_in_months_and_heights)\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["##### Wykorzystanie dowolnego algorytmu z Spark MLlib\n\nDokonano klasteryzacji z wykorzystaniem metody k-średnich."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.clustering import KMeans, KMeansModel\nfrom numpy import array\nfrom math import sqrt\n\n# bierzemy najnizszy pomiar dla kazdego dnia\nreducedMeasures = measures.map(lambda x: (x[9],x,)).reduceByKey(lambda x, y: x if x[4] < y[4] else y)\n#[x for x in reducedMeasures.toLocalIterator()]\n\n# do k-srednich wrzucamy temperature i punkt rosy\nmeasures_for_kmeans = reducedMeasures.map(lambda x: array([float(x[1][5]), float(x[1][7])]))\n#[x for x in measures_for_kmeans.toLocalIterator()]\n\n# dzielimy na 2 grupy umownie lato i zima\nclusters = KMeans.train(measures_for_kmeans, 4, maxIterations=10, initializationMode=\"random\")\n\ndef error(point):\n    center = clusters.centers[clusters.predict(point)]\n    return sqrt(sum([x**2 for x in (point - center)]))\nWSSSE = measures_for_kmeans.map(lambda point: error(point)).reduce(lambda x, y: x + y)\nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n\n# laczymy wyniki klasteryzacji z danymi (dokonujemy predykcji z modelu)\npredictions = reducedMeasures.map(lambda x: (x[1][0], x[1][1], x[1][2], x[1][3], x[1][4], x[1][5], x[1][6], x[1][7], x[1][8], x[1][9], clusters.predict(array([x[1][5],x[1][7]]))))\n\npredictionsDF = spark.createDataFrame(predictions,['year', 'month', 'day', 'pressure', 'height', 'temp', 'dewpoint', 'humidity', 'height_group', 'datestamp', 'CLUSTER'])\npredictionsDF.orderBy('year', 'month', 'day', ascending=True).show()\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# wyswietlamy rezultat dla konkretnego roku\n#display(predictionsDF.filter(predictionsDF.year == 2014).orderBy('month', 'day', ascending=True))\ndisplay(predictionsDF.filter(predictionsDF.year == 2015).orderBy('month', 'day', ascending=True))\n#display(predictionsDF.filter(predictionsDF.year == 2016).orderBy('month', 'day', ascending=True))\n\n# ustawic datestamp/cluster/temp i jako wykres obszarowy"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"meteo","notebookId":3720619873347694},"nbformat":4,"nbformat_minor":0}
